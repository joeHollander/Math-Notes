\documentclass{article}
\usepackage{graphicx}
\usepackage{physics}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage{titlesec}

\title{Investment Management Course Notes}
\author{Joe Hollander}
\date{Summer 2024} 

\begin{document}
\maketitle


\section*{1.1 Fundamentals of risk and returns}

Compounding returns with different return rates:
\[
(1 + r_1)(1 + r_2) - 1.
\]
Compounding returns with the same return rate:
\[
((1 + r)^t - 1). 
\]
To compare different time period standard deviations multiply (or divide)
by the square root of the number of time periods.
The sharpe ratio:
\[
\frac{R_p-R_f}{\sigma_p}.
\]

Pandas standard deviation method uses the sample
standard deviation and not the population standard
deviation. Similar to the sharpe ratio, the calmar
ratior is a risk adjusted return where risk is measured
by drawdown. Use index method to\_period to convert from
datetime to period. Use series method cummax to find the
highest value for each timestep. Drawdown is then: 
\[
\frac{Value - Previous Peak}{Previous Peak}.
\]

\section*{1.2 Beyond The Gaussian Case}

Skew and kurtosis are the third and fourth moments
of a distribution respectively. A distribution with
a greater than three kurtosis is considered fat tailed.

The Jarque Bera is a test that determines whether 
or not a sample distribution fits a normal distribution
based on its skew and kurtosis. If the test is close
to zero it signals that the distribution is close 
to normal. The test:
\[
JB = \frac{n}{6}\left(S^2 + \frac{1}{4}(K-3)^2\right).
\]

Semi-deviation is the volatility of below-average
or below-zero returns:
\[
\sigma_{semi} = 
\sqrt{\frac{1}{N}\sum_{R_t\le\bar{R}}(R_t-\bar{R})^2}.
\]
Value at risk (VaR) represents the maximum
"expected" loss over a time period. A 99\%
one month VaR gives the maximum loss excluding
the 1\% of worst cases. The VaR is also typically
expressed as a positive number. The conditional
value at risk (CVaR) is the expected loss beyond
VaR:
\[
CVar = -E(R \, | \, R \le -VaR).
\]

Setting ddof as zero for pandas standard deviation
calculates the population standard deviation.

There are four methods to calculate VaR.
The historical methodology calculates the VaR 
from the historical outcomes. The parametric gaussian
methodology assumes a gaussian distribution. In
this methodology the VaR is simple to calculate:
\[
VaR_\alpha = -(\mu + z_\alpha \sigma),
\]
where $z_\alpha$ is the $\alpha$-quantile of the
normal distribution with mean zero and standard
deviation one. The gaussian is almost inaccurate.
A parametric non-gaussian distribution doesn't
assume gaussian. The Cornish-Fisher VaR is a
semi-parametric approach. The expansion states:
\[
\tilde{z_\alpha} = z_\alpha + 
\frac{1}{6}(z_\alpha^2 - 1)S + 
\frac{1}{24}(z_\alpha^3 - 3z_\alpha)(K-3) -
\frac{1}{36}(2z_\alpha^3 - 5z_\alpha)S^2,
\] 
where $\tilde{z_\alpha}$ is the updated quantile.

Use numpy's percentile for historical VaR and
scipy.stats' ppf function for gaussian, parametric,
and Cornish-Fisher VaR.

\section*{2.1 Optimization and the Efficient Frontier}

The return of a portfolio is equal to the 
weighted average of the return of the components.
The volatility of a portfolio, however, depends
on the correlation:
\[
\sigma^2(w_a, w_b) =
\sigma^2_A w^2_A + \sigma^2_B w^2_B + 
2 w_A w_B \sigma_A \sigma_B \rho_{A,B}.
\]
The efficient frontier is the boundary of regions
created from the assets with a given correlation.
Each point on this line represents the best return
for each volatility. 

\section*{2.2 Implementing Markowitz}

The capital market line is the tangent line
from the risk free rate to the efficient frontier.
The portfolios from this line have the highest 
sharpe ratio. This maximum sharpe ratio portfolio (MSR)
also has no exposure to unrewarded risks. 

As a result of estimation errors and misleading
expected returns, some use the global minimum variance
(GMV), which is the nose of the efficient frontier. 
Even small estimation errors result in large portfolio
changes. 

\section*{3.1 CPPI and Drawdown constraints}

You cannot diversify out all risk. Dynamic
hedging allows for upside exposure with 
less downside exposure. Correlation often rises when
returns fall. 

The CPPI procedure allows for the construction 
of convex payoffs. The risky asset is a multiplier multiplied by
the cushion. The CPPI is the riskless asset plus the
risky asset. The cushion is then the CPPI minus the protection
floor set in place. Therefore, as the cushion decreases, more
of the portfolio is riskless assets, and as the cushion increases,
more of the portfolio is risky assets. 

\begin{center}
    \includegraphics[width=12cm,height=12cm, keepaspectratio]{CPPI.png}
\end{center}

\noindent Gap risk occurs when trading discretely.

Given a max drawdown contraint 
\[V_t > \alpha M_t,\] where: $V_t$ is the value of the portfolio,
$M_t$ is the peak of the portfolio between time $0$ and time $t$,
and $1-\alpha$ is the maximum acceptable drawdown. Then choosing
a multiplier multiplied by $M_t \alpha$ will also provide a convex
payoff. 

A cap can also be used to reduce risk taking passed a value. 
In this system, if the floor is closer, the distance to the floor
is used while if the ceiling cap is closer, the distance to the
ceiling is used. 

Instead of constructing a new DataFrame using an array,
use the DataFrame method reindex\_like.

We can model the return process of a stock $S_t$
with risk-free rate $r$, sharpe ratio $\lambda$,
and volatility $\sigma$: 
\[
\frac{dS_t}{S_t} = 
(r + \sigma \lambda)dt + \sigma dW_t.
\]
For discrete time:
\[
\frac{S_{t+dt} - S_t}{S_t} =
(r + \sigma \lambda)dt +
\sigma \sqrt{dt} \, \xi_t.
\]

\section*{3.2 Monte Carlo}

A more general model of a return process
with the same variables: 
\[
\frac{dS_t}{S_t} = 
\left(r_t + \sqrt{V_t} \, \lambda_t^S\right)dt +
\sqrt{V_t} \, dW_t^S.
\]

We can also define the risk-free rate and variance
in terms of brownian motion: 
\begin{gather}
    dr_t = a(b-r_t)dt + \sigma_r dW_t^r \nonumber \\
    dV_t = \alpha(\bar{V} - V_t)dt + 
    \sigma_V \sqrt{V_t} \, dW_t^V \nonumber
\end{gather}
Here, $b$ is the long term mean of the risk-free rate.
Both of these processes are mean reverting. 

In a CPPI system, raising the risky-asset multiplier
when the market is less volatile and vice-versa results
in less breaches of the floor and less need for 
rebalancing more frequently. 

\section*{4.1 Asset-Liability Management}

The funding ratio $F_t = {A_t}/{L_t}$ and surplus
$S_t = A_t - L_t$ is what really matters
for asset-liability management.  

The present value of a set of liabilities is
\[
PV(L) = \sum_{i=1}^k B(t_i)L_i,
\]
and if the yield curve is flat, 
the price of a pure discount bond is 
\[
B(t) = \frac{1}{(1+r)^t}, 
\]
where $r$ is the annual rate of interest. 

Liability-hedging portfolios attempt to match the cashflows of
the liability side in order to pay liabilities in the future. 
Often, cash-flow matching is not feasible or practicle, so one may
use factor exposure matching. These factor exposure matching portfolios
often use bonds to gain similar exposure to interest rates that affect
liabilities. 

The Cox Ingersoll Ross (CIR) model that is used to model interest rates:
\[
dr_t = a(b-r_t)dt + \sigma \sqrt{r_t} \, dW_t,
\]
where $b$ is the long term mean of the interest rate. 

Since performance generation and hedging are incompatible aims,
creating a performance-seeking portfolio (PSP) and
a liability-hedging portfolio (LHP) is the best option.
This two portfolio style is called liability-driven investing (LDI).
The formal expression of the goal is:
\[
\max_w E \left[u\left(\frac{A_t}{L_t}\right)\right]
=> w^* = \frac{\lambda_{PSP}}{\gamma \sigma_{PSP}}w^{PSP}
+ \beta_{L,LHP} \left(1-\frac{1}{\gamma}\right)
w^{LHP}.
\]
The greeks of LDI are these greek letters. $\lambda_{PSP}$ 
is the sharpe ratio of the PSP, $\gamma$ is the risk aversion,
$\sigma_{PSP}$ is the standard deviation of the PSP,
$\beta_{L,LHP}$ is the beta of the liabilities with respect to the LHP.

\section*{1.2 Introduction to Machine Learning}

Aqequate data is needed for a train-test split, and
data must be stable (stationary). 
While traditional statistics builds model and relies
on assumptions, machine learning relies on large data. 

Taking the average classification of multiple classifiers
does best with respect to reducing estimation errors.
Implementing a voting system, called boosting, is another way
to combine different classifiers. 

Dimensional reduction such as PCA helps with feature selection.
Clustering algorithms can also be used. 

\section*{2.1 Introduction to Factor Models}

Macro-factors such as GDP growth, interest rate, and inflation
are applicaple to many asset classes. Micro-factors, on the 
other hand, are only applicable to specific asset classes. 
Investing with an aim for diverse factor exposure is better than
investing for general diversification as different asset classes
may have the same factors. 





\end{document}