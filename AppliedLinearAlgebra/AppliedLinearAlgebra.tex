\documentclass{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathtools}

\newcommand\setItemnumber[1]{\setcounter{enumi}{\numexpr#1-1\relax}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\colvec}{\begin{bmatrix} x \\ y \end{bmatrix}}

\title{Applied Linear Algebra Notes}
\author{Joe Hollander}
\date{Summer 2025}  

\begin{document}
\maketitle 

\section{}
    \subsection*{1.1}
    Elementary Row Operations
    \begin{enumerate}
        \item Interchange two rows.
        \item Multiply the elements of a row by a nonzero constant.
        \item Add a multiple of the elements of one row to the corresponding elements of another.
    \end{enumerate}
    Gauss-Jordan elimination is a systematic way to eliminate variables to arrive at a solution matrix. 
    The equation is solved when the matrix is diagonalized. Create zeros in each column until finished. 
    Finally, normalize the final element ((3,3) for a 3x3 matrix, etc.). 
    Interchange rows if a diagonal element is zero. Final matrix is called the reduced echelon form.
    The formula can be used for multiple constants with the same coefficient matrix. 
    To do this, just stack the constants in each column: 
    \begin{gather*}
    x - y + 3z = b_1 \\
    2x - y + 4z = b_2 \\
    -x + 2y -4z = b_3
    \end{gather*}
    
    for 
    \[
    \begin{bmatrix}
        b_1 \\
        b_2 \\
        b_3 \\
    \end{bmatrix}
    =
    \begin{bmatrix}
        8 \\
        11 \\
        -11 \\
    \end{bmatrix},
    \begin{bmatrix}
        0 \\
        1 \\
        2 \\
    \end{bmatrix},
    \begin{bmatrix}
        3 \\
        3 \\
        4 \\
    \end{bmatrix}
    \]
    would produce the augmented matrix
    \[
    \begin{bmatrix}
        1 & -1 & 3 & 8 & 0 & 3 \\
        2 & -1 & 4 & 11 & 1 & 3 \\
        -1 & 2 & -4 & -11 & 2 & 4 \\
    \end{bmatrix}
    \]
    which simplifies to 
    \[
    \begin{bmatrix}
        1 & 0 & 0 & 1 & 0 & -2 \\
        0 & 1 & 0 & -1 & 3 & 1 \\
        0 & 0 & 1 & 2 & 1 & 2 \\
    \end{bmatrix}.
    \]

    \subsection*{1.2}
    A matrix is in reduced echelon form if:
    \begin{enumerate}
        \item Any rows consisting entirely of zeros are grouped at the bottom of the matrix.
        \item The first nonzero element of each other row is 1. This element is called a leading 1. 
        \item The leading 1 of each row after the first is positioned to the right of the leading 1 of the previous row.
        \item All other elements in a column that contain a leading 1 are zero. 
    \end{enumerate}
    The reduced echelon of a matrix is unique. 
    To express multiple solutions, write the leading varaibles in each equation 
    (variables with the lowest subscript: $x_1$, etc.)  in terms of the remaining free variables.
    If last row has all zeros apart from constant, there are no solutions for the system.
    A system of linear equations is said to be homogeneous if all the constants are zero.  
    A homogeneous system of linear equations always has a solution of all zeros. 
    A homogenous system of linear equations that has more variables than equations (more rows than columns) 
    has many solutions. 

    \subsection*{1.7}
    Curve fitting a polynomial involves estimating variables, which can be done by solving a system of linear equations.
    Kirchhoff's Laws state:
    \begin{enumerate}
        \item All the current flowing into a junction must flow out of it
        \item The sum of the IR terms (I denotes current, R resistance) in any direction around a closed path
        is equal to the total voltage in the path in that direction. 
    \end{enumerate}
    Using these laws a system of linear equations can be formed to determine the current flowing through certain points.
    The same procedure can be used with traffic. 

    \subsection*{1.3}
    $\mathbb{R}^n$ is closed under addition and scalar multiplication if the result is also in $\mathbb{R}^n$. 
    To determine if $\mathbf{r}$ is a linear combination of $\mathbf{u, v,}$ and $\mathbf{w}$, 
    determine if this system has a solution: 
    \begin{gather*}
        c_1\mathbf{u_1} + c_2\mathbf{v_1} + c_3\mathbf{w_1} = \mathbf{r_1} \\
        c_1\mathbf{u_2} + c_2\mathbf{v_2} + c_3\mathbf{w_2} = \mathbf{r_2} \\
        c_1\mathbf{u_3} + c_2\mathbf{v_3} + c_3\mathbf{w_3} = \mathbf{r_3}.
    \end{gather*}
    The norm of a vector $\mathbf{r}$ is $\norm{\mathbf{r}}$, and it denotes the magnitude of a vector. 
    In $\mathbb{R}^n$, $\norm{\mathbf{r}} = \sqrt{\mathbf{r}^n}$.

    \subsection*{1.4}
    Subspaces are subsets of $\mathbb{R}^n$ that have all the same properties (subset must be closed under addition and multiplication).
    An example for demonstrating a subset in $\mathbb{R}^3$ with the form $(a, 2a, 3a)$ is closed for addition: 
    \begin{align*}
       (a, 2a, 3a) + (b, 2b, 3b) &= (a + b, 2a + 2b, 3a + 3b) \\
       &= (a + b, 2(a +b), 3(a + b)), 
    \end{align*}
    and multiplication: 
    \begin{align*}
        k(a, 2a, 3a) &= (ka, 2ka, 3ka) \\
        &= (ka, 2(ka), 3(ka)).
    \end{align*}


    When every vector of a vector space can be written as a linear combination of a set of vectors, the vectors span the space.
    For example, the subspace in $\mathbb{R}^n$ defined by $(a, b, a+b)$ is spanned by the vectors $(1, 0, 1)$ and $(0, 1, 1)$
    because $(a, b, a+b) = a(1, 0, 1) + b(0, 1, 1)$. Generally, vectors that make up a line in $\mathbb{R}^n$ are subspaces.
    The set of solutions to any homogenous system of linear equations is a subspace.

    \subsection*{1.5}
    The basis is the finite set of vectors that span a whole space. 
    The basis must also be linearly independent. 
    The vectors $\mathbf{v_1, v_2, \dots, v_n}$ are linearly independent 
    if the identity $c_1 \mathbf{v_1} + c_2 \mathbf{v_2} + \dots + c_n \mathbf{v_n} = \mathbf{0}$ 
    is only true for $c_1 = 0, c_2 = 0, \dots, c_n = 0$. 
    If the vectors are not linearly independent, they are linearly dependent. 
    Vectors can span the space but not be linearly independent. 
    For example, the vectors $(0, 1), (1, 0)$, and $(1, 1)$ span $\mathbb{R}^2$,
    but the set of the vectors is not a basis for $\mathbb{R}^2$ because they are not linearly independent.
    Additionally, $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$. 
    All subspaces must pass through the origin because they must contain the $\mathbf{0}$ vector. 

\section{}
    \subsection*{2.1}
    Matrix notation:
    \[
    A = 
    \begin{bmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33} \\
    \end{bmatrix}.
    \]
    The main diagonal consists of $a_{11}, a_{22}$, and $a_{33}$.
    Addition involves adding the corresponding elements of each matrix. 
    Both matrices must be the same size.
    Scalar multiplication involves multiplying every element. 
    Matrix multiplication ($C = AB$) requires that the number of columns in matrix $A$,
    is the same as the number of rows in matrix $B$. 
    The product is equal to the product of the rows of $A$ and the columns of $B$:
    \[
    c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \dots + a_{in}b_{nj}.
    \]
    For example, 
    \begin{align*}
    AB &= 
    \begin{bmatrix}
        1 & 3 \\
        2 & 0
    \end{bmatrix}
    \begin{bmatrix}
        5 & 0 & 1 \\
        3 & -2 & 6 
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 5 \\ 3 \end{bmatrix} & \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 0 \\ -2 \end{bmatrix} & \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 6 \end{bmatrix} \\
        \begin{bmatrix} 2 & 0 \end{bmatrix} \begin{bmatrix} 5 \\ 3 \end{bmatrix} & \begin{bmatrix} 2 & 0 \end{bmatrix} \begin{bmatrix} 0 \\ -2 \end{bmatrix} & \begin{bmatrix} 2 & 0 \end{bmatrix} \begin{bmatrix} 1 \\ 6 \end{bmatrix}
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        (1)(5) + (3)(3) & (1)(0) + (3)(-2) & (1)(1) + (3)(6) \\
        (2)(5) + (0)(3) & (2)(0) + (0)(-2) & (2)(1) + (0)(6)
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        14 & -6 & 19 \\
        10 & 0 & 2
    \end{bmatrix}.
    \end{align*}
    Matrix multiplication is not commumative, and $BA$ does not exist.
    If $A$ is an $m \times r$ matrix and $B$ is an $r \times n$ matrix, 
    then $AB$ is an $m \times n$ matrix.
    Three classes of matrices are notable: 
    The zero matrix $O$ which contains all zeros, 
    the diagonal matrix which contains zeros everywhere except the main diagonal,
    and the identity matrix $I$ which is a diagonal matrix with ones on the main diagonal.
    If $B_1$ is column one of matrix $B$,
    then $AB = [AB_1, AB_2, \dots, AB_n]$.
    Matrices can be partitioned into blocks, and these blocks can be multiplied.

    \subsection*{2.2}
    For matrix multiplication,
    \begin{enumerate}
        \item $A(BC) = (AB)C$
        \item $A(B + C) = AB + AC$
        \item $(A + B)C = AC + BC$
    \end{enumerate}
    The product of a chain of matrices will have the same number of rows as the first matrix 
    and the same number of columns as the last matrix. 
    For matrices, $AB = AC$ doesn't imply $B = C$, and
    $PQ = O$ doesn't imply $P = O$ or $Q = O$, 
    If $A$ is an $n \times n$ square matrix, and $r$ and $s$ are nonnegative integers, then
    \begin{enumerate}
        \item $A^rA^s = A^{r+s}$
        \item $(A^r)^s = A^{rs}$
        \item $A^0 = I_n$
    \end{enumerate}

    Consider a homogenous system of linear equations $AX = 0$. 
    Let $X_1$ and $X_2$ be solutions of the system.
    Then $AX_1 = 0$ and $AX_2 = 0$, so $AX_1 + AX_2 = A(X_1 + X_2) = 0$, and $X_1 + X_2$ is also a solution.
    Furthermore, if $c$ is any scalar, then $AX_1 = 0$ and $c(AX_1) = A(cX_1) = 0$, so $cX_1$ is also a solution.

    The corresponding homogeneous system of a matrix is one in which the right-hand side of the equations is zero,
    and the solution of the corresponding nonhomogeneous system is the solution of the homogeneous system plus a particular solution of the nonhomogeneous system.

    \subsection*{2.3}
    The transpose of a matrix $A$, denoted $A^T$, is the matrix whose columns are the rows of the given matrix $A$.
    Let $A$ and $B$ be matrices and $c$ be a scalar. There are several properties of the transpose:
    \begin{enumerate}
        \item $(A + B)^T = A^T + B^T$
        \item $(cA)^T = cA^T$
        \item $(AB)^T = B^TA^T$
        \item $(A^T)^T = A$
    \end{enumerate}
    A symmetrical matrix is a matrix that is equal to its transpose.
    The trace of $A$, denotedenoted $tr(A)$, is the sum of the elements on the main diagonal.
    There are several properties of the trace:
    \begin{enumerate}
        \item $tr(A + B) = tr(A) + tr(B)$
        \item $tr(AB) = tr(BA)$
        \item $tr(cA) = ctr(A)$
        \item $tr(A^T) = tr(A)$
    \end{enumerate}
    Matrices can also contain complex numbers, 
    and the conjugate of a matrix $A$ is denoted $\bar{A}$.
    Furthermore, the conjugate transpose of a matrix $A$ is denoted $A^*$,
    and a square matrix $C$ is called Hermitian if $C = C^*$.
    There are several properties of the conjugate transpose:
    \begin{enumerate}
        \item $(A + B)^* = A^* + B^*$
        \item $(zA)^* = \bar{z}A^*$
        \item $(AB)^* = B^*A^*$
        \item $(A^*)^* = A$
    \end{enumerate}

    In archeological seriation, a matrix is used to represent the relative ages of artifacts,
    with the defintion 
    \[
    a_{ij} = \begin{cases}
        1 & \text{if grave} \ i \ \text{contains pottery type} \ j \\
        0 & \text{if grave} \ i \ \text{does not contain pottery type} \ j
    \end{cases}
    \] 
    The element $g_{ij}$ of the matrix $G = AA^T$ 
    is equal to the number of types of pottery commmon to both grave $i$ and grave $j$.
    Then, create a map of the ages of the graves by starting with a great element from this matrix,
    and create connections depending on the elements. 
    For example, consider the matrix 
    \[
    G = \begin{bmatrix}
        1 & 1 & 0 & 0 & 0 \\
        1 & 2 & 0 & 0 & 1 \\
        0 & 0 & 2 & 1 & 1 \\
        0 & 0 & 1 & 1 & 0 \\
        0 & 1 & 1 & 0 & 2
    \end{bmatrix}
    \]
    Since $g_{12} = 1$, graves 1 and 2 share one type of pottery, so they are connected, and our current map is $1-2$. 
    Similarly, $g_{25} = 1$, so graves 2 and 5 are connected, and our map is now $1-2-5$.
    Again, $g_{53} = 1$, so graves 5 and 3 are connected, and our map is now $1-2-5-3$.
    Finally, $g_{34} = 1$, so graves 3 and 4 are connected, and our final map is now $1-2-5-3-4$.
    This means that grave 1 is the oldest or that grave 4 is the oldest. 
    In a similar method, the matrix $P=A^TA$ can be constructed, 
    and its elements detail how many graves contain both the $i$th and $j$th types of pottery. 
    The chronological order of the pottery can then also be determined using a map. 

    \subsection*{2.4}
    Let $A$ be an $n \times n$ matrix. If a matrix $B$ can be found such that $AB = BA = I_n$, 
    then $A$ is said to be invertible, and $B$ is called the inverse of $A$.
    If $A$ is invertible, then the unique inverse of $A$ is denoted $A^{-1}$.
    $A^{-k}$ is defined as $(A^{-1})^k$. 
    The inverse of matrix can be found by solving the system of linear equations $AX = I_n$
    For example, to determine the inverse of the matrix
    \[
    A = 
    \begin{bmatrix}
        1 & -1 & -2 \\
        2 & -3 & -5 \\
        -1 & 3 & 5
    \end{bmatrix},
    \]
    determine the reduced echelon form of the augmented matrix
    \[
    [A:I_n] =
    \begin{bmatrix}
         1 & -1 & -2 & 1 & 0 & 0 \\
        2 & -3 & -5 & 0 & 1 & 1\\
        -1 & 3 & 5 & 0 & 0 & 1
    \end{bmatrix}.
    \]
    There are several properties of the matrix inverse:
    \begin{enumerate}
        \item $(A^{-1})^{-1} = A$
        \item $(cA)^-1 = \frac{1}{c}A^{-1}$
        \item $(AB)^{-1} = B^{-1}A^{-1}$
        \item $(A^n)^{-1} = (A^{-1})^n$
        \item $(A^T)^{-1} = (A^{-1})^T$
    \end{enumerate}

    Let $AX=Y$ be a system of $n$ linear equations in $n$ variables. 
    If $A$ is invertible, then $X = A^{-1}Y$. 
    This method is often not used, for it is inefficient. 

    An elementary matrix is one that can be obtained from the identity matrix by a single elementary row operation.
    There are three types of elementary matrices:
    \begin{enumerate}
        \item Interchange rows
        \item Multiply a row by a nonzero constant
        \item Add a multiple of one row to another row
    \end{enumerate}
    These matrices can be used to apply elementary row operations to matrices through matrix multiplication. 
    Each elementary matrix is square and invertible,
    and if $A$ and $B$ are row equivalent matrices (one can be obtained from the other by elementary row operations) 
    and $A$ is invertible, then $B$ is also invertible.
    These matrices are used in the LU factorization of a matrix.


    \subsection*{2.5}
    A transformation $T$ of $\mathbb{R}^n$ into $\mathbb{R}^m$ is a rule that assigns to each vector $\mathbf{u}$ in $\mathbb{R}^n$
    a unique vector $\mathbf{u}$ in $\mathbb{R}^m$, and is denoted by $T(\mathbf{u}) = \mathbf{v}$.
    $\mathbb{R}^n$ is the domain of $T$, and $\mathbb{R}^m$ is the codomain of $T$.
    There are three main types of transformations:
    \begin{enumerate}
        \item Dilation: $T(\colvec) = \begin{bmatrix} r & 0 \\ 0 & r \end{bmatrix}\colvec$
        \item Reflection: $T(\colvec) = \begin{bmatrix} \pm1 & 0 \\ 0 & \pm1 \end{bmatrix}\colvec$
        \item Rotation: $T(\colvec) = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}\colvec$
    \end{enumerate}
    Transformations can also be combined. 

    An orthogonal matrix $A$ is an invertible matrix that has the property $A^-1 = A^T$.
    An orthogonal transformation is one where $T(\mathbf{u}) = A\mathbf{u}$ for some orthogonal matrix $A$.
    Orthogonal transformations preserve norms, differences in angles, and distances (rigid motions).

    A translation is a transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ of the form 
    $T(\mathbf{u}) = \mathbf{u} + \mathbf{v}$, where $\mathbf{v}$ is a fixed vector.
    An affine transformation is a transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ of the form
    $T(\mathbf{u}) = A \mathbf{u} + \mathbf{v}$. 

    \subsection*{2.6}
    Linear transformations imply that $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$ and
    $T(c\mathbf{u}) = cT(\mathbf{u})$ for any vectors $\mathbf{u}$ and $\mathbf{v}$ and any scalar $c$.
    Affine transformations are not linear transformations.
    To prove that a transformation is linear, it is sufficient to show that the transformation preserves vector addition and scalar multiplication.
    Any linear transformation can be represented by its standard matrix
    a matrix whose columns are the images of the standard basis vectors: 
    \[A = [T(\mathbf{e}_1) \ \dots \ T(\mathbf{e}_n)]\]
    To determine the transformation matrix then, apply the transformation to the standard basis vectors.
    
    Typically, translations cannot be used in combination with linear transformations as a product.
    However if an extra dimension is added, the translation can be represented as a linear transformation.
    This is called an homogeneous coordinate system. 
    For example, a point is represented as 
    \[
    \begin{bmatrix}
        x \\
        y \\
        1
    \end{bmatrix},
    \]
    and a translation is represented as
    \[
    \begin{bmatrix}
        1 & 0 & a \\
        0 & 1 & b \\
        0 & 0 & 1
    \end{bmatrix},
    \]
    where $a$ and $b$ are the translation amounts.

    Fractals can be generated using multiple affine transformations given various probabilities. 
    These parameters can be written as rows of a iterated function system matrix.
    
    \subsection*{2.8}
    A stochastic matrix is a square matrix whose elements are probabilities and whose columns sum to 1.
    If $A$ and $B$ are stochastic matrices, then $AB$ is also a stochastic matrix.
    
    \section*{3}

    \subsection*{3.1}
    The determinant of a $2 \times 2$ matrix $A$ is denoted by $|A|$ or det($A$) and is given by 
    \[
    \begin{vmatrix}
        a_{11} & a_{12} \\
        a_{21} & a_{22}
    \end{vmatrix}
    = 
    a_{11}a_{22}-a_{12}a_{21}
    \]
    This determinant is given by the difference of the products of the two diagonals of the matrix. 

    Let $A$ be a square matrix. The minor of the element $a_{ij}$ is the determinant of the matrix 
    obtained by removing the $i$th row and $j$th column from $A$.
    The cofactor of the element $a_{ij}$ is denoted $C_{ij}$ and is given by
    \[
    C_{ij} = (-1)^{i+j}M_{ij}
    \]
    Now, a new definition. 
    The determinant of a square matrix is the sum of the products of the elements of the first row
    and their cofactors. For example, if $A$ is a $3 \times 3$ matrix,
    then $|A| = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13}.$
    More generally, the determinant of a square matrix is the sum of the products
    of the elements of any row or column and theri cofactors. 
    \[
    i\text{th row expansion:} \quad |A| = a_{i1}C_{i1} + a_{i2}C_{i2} + \dots + a_{in}C_{in} \\
    \]
    The computation in evaluating a determinant can be minimized 
    by expanding in terms of the row or column that contains the most 0's. 
    The determinant of a $3 \times 3$ matrix can also be found using diagonals. 
    \[
    \begin{vmatrix}
    a & b & c \\
    d & e & f \\
    g & h & i
    \end{vmatrix}
     = aei + bfg + cdh - ceg - bdi - afh
    \]

    \subsection*{3.2}
    Let $A$ be a square matrix and $c$ be a nonzero scalar.
    \begin{enumerate}
        \item If a matrix $B$ is obtained from $A$ by multiplying a row or column by $c$, then $|B| = c|A|$.
        \item If a a matrix $B$ is obtained from $A$ by interchanging two rows or columns, then $|B| = -|A|$.
        \item If a matrix $B$ is obtained from $A$ by adding a multiple of one row or column to another, then $|B| = |A|$.
    \end{enumerate}

    A square matrix $A$ is singular if $|A| = 0$, and $A$ is nonsingular if $|A| \neq 0$.
    Let $A$ be a square matrix. A is singular if 
    \begin{enumerate}
        \item all the elements of a row or column are zero
        \item two rows or columns are equal
        \item twos rows or columns are proportional
    \end{enumerate}
    Let $B$ be a matrix with the same size as $A$.
    $AB$ being singular implies that either one of or both of $A$ and $B$ are singular, 
    but the converse is not true.

    Let $A$ and $B$ be $n \times n$ matrices and $c$ be a nonzero scalar.
    \begin{enumerate}
        \item $|cA| = c^n|A|$
        \item $|AB| = |A||B|$
        \item $|A^T| = |A|$
        \item $|A^{-1}| = \frac{1}{|A|}$
    \end{enumerate}

    The determinant of a triangular matrix is equal to the product of the diagonal elements.
    When simplifying to find the determinant, if at any stage one of the diagonal elements is zero, 
    and all elements below it are zero, then the determinant is zero.

    \subsection*{3.3}
    The matrix whose elements are cofactors of $A$ is called the matrix of cofactors of $A$.
    The transpose of this matrix is called the adjoint of $A$ and is denoted adj($A$).
    Let $A$ be a nonsingular square matrix. 
    \[
    A^{-1} = \frac{1}{|A|}adj(A)
    \]
    Furthermore, $A^{-1}$ exists if and only if $|A| \neq 0$.
    
    Let $AX = B$ be a system of $n$ linear equations in $n$ variables. 
    If $A$ is nonsingular, then $X = A^{-1}B$, but if $A$ is singular, then there are many or no solutions.
    Cramer's rule states that if $A$ is nonsingular, then the system has a unique solution given by
    \[
    x_1 = \frac{|A_1|}{|A|}, \quad \dots, \quad x_n = \frac{|A_n|}{|A|}
    \]
    where $A_i$ is the matrix obtained by replacing column $i$ of $A$ with B.
    Since a nonsingular matrix can only have one solution,
    a homogeneous system of linear equations can only have a solution apart from the trivial solution
    if the determinant of the matrix of coefficients is zero. 

    \subsection*{3.4}
    Let $A$ be a $n \times n$ matrix. 
    A scalar $\lambda$ is called an eigenvalue of $A$ 
    if there exists a nonzero vector $\mathbf{x}$ in $\mathbb{R}^n$ such that
    \[
    A\mathbf{x} = \lambda \mathbf{x}.
    \]
    The vector $\mathbf{x}$ is called an eigenvector of $A$ corresponding to $\lambda$.
    Solving $|A - \lambda I_n| = 0$ for $\lambda$ leads to all the eigenvalues of $A$.
    The expansion of $A - \lambda I_n$ gives the characteristic polynomial,
    a polynomial of degree $n$ in $\lambda$ that has $\lambda$ as a root.
    Inputting the found $\lambda$ back in to $(A - \lambda I_n)\mathbf{x}$ 
    gives the corresponding eigenvectors (there are an infinite amount for each eigenvalue).
    The set of all eigenvectors corresponding to a given eigenvalue 
    forms a subspace of $\mathbb{R}^n$ called the eigenspace of $A$. 
    The number of times an eigenvalue appears as a root of the characteristic polynomial 
    is called the  multiplicity of the eigenvalue.

    In a triangular matrix, the eigenavalues are the diagonal elements of the matrix.

    \subsection*{3.5}
    The transition matrix $P$ of a Markov chain is said to be regular if for some power of $P$ all the
    components are positive. 
    The chain is then called a regular Markov chain.
    Define the limit of a regular Markov chain with transition matrix $P$ as $\mathbf{x}$. 
    Then, $\mathbf{x}$ is an eigenvector of $P$ corresponding to $\lambda = 1$,
    and the transition matrix also approaches a stochastic matrix with identical columns,
    each being an eigenvector of $P$ corresponding to $\lambda = 1$.
    
    Leslie matrices are a discrete, age-structured model of population growth.
    They also have eigenvectors that represent the long-term population distribution
    and eigenvalues that represent the long-term growth rate.

<<<<<<< HEAD
    \subsection*{5.3}
    $B$ is similar to $A$ if there exists an invertible matrix $C$
    such that $B = C^{-1}AC$.
    Similar matrices have the same eigenvalues. 
    Let $A$ be an $n \times n$ matrix. 
    \begin{enumerate}
        \item If $A$ has $n$ linearly independent eigenvectors, it is diagonalizable.
        The matrix $C$ whose columns consist of $n$ linearly independent eigenvectors 
        can be used in a similarity transformation to give a diagonal matrix $D$. 
        This diagonal elements of $D$ will be the eigenvalues of $A$.
        \item If $A$ is diagonizable, then it has $n$ linearly independent eigenvectors.
    \end{enumerate}


    Let
    \[
    A = 
    \begin{bmatrix}
        -4 & -6 \\
        3 & 5 \\
    \end{bmatrix}.
    \]
    This matrix has eigenvalues 2 and -1, 
    corresponding to eigenvectors $\begin{bmatrix} -1 \\ 1 \end{bmatrix}$ and $\begin{bmatrix} -2 \\ 1 \end{bmatrix}$. 
    The diagonal elements of the diagonalized matrix are the eigenvalues
    \[
    D = 
    \begin{bmatrix}
        2 & 0 \\
        0 & -1 \\
    \end{bmatrix},
    \]
    and the column vectors of the diagonalizing matrix are linearly independent eigenvectors
    \[
    C = 
    \begin{bmatrix}
        -1 & -2 \\
        1 & 1 \\
    \end{bmatrix}.
    \]
    The order of the eigenvalues in the diagonalized matrix $D$
    is the same as the order of the eigenvectors in the diagonalizing matrix $C$.

    This makes computation easy because $A^k = CD^kC^{-1}$. 

    Let $A$ be an $n \times n$ symmetric matrix. 
    \begin{enumerate}
        \item If $A$ is real, all of the eigenvalues are real, and it has $n$ linearly independent eigenvectors.
        \item The dimension of the eigenspace corresponding to an eigenvalue is equal to the multiplicity of the eigenvalue.
        \item The eigenspaces of $A$ are orthogonal. 
    \end{enumerate}

=======
    \subsection*{4.1}
    A vector space is a set $V$ of elements called vectors,
    having operations of addition and scalar multiplication
    defined on it tht satisfy the following conditions:
    \begin{enumerate}
        \item The sum $\mathbf{u} + \mathbf{v}$ exists and is an element of $V$.
        \item $c \mathbf{u}$ exists and is an element of $V$.
        \item $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$
        \item $\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$
        \item The zero vector $\mathbf{0}$ exists and is an element of $V$.
        \item For every element $\mathbf{u}$ of $V$, $\mathbf{-u}$ exists such that $\mathbf{-u} + \mathbf{u} = \mathbf{0}$.
        \item $c(\mathbf{u} + \mathbf{v}) = c\mathbf{u} + c\mathbf{v}$
        \item $(c + d)\mathbf{u} = c\mathbf{u} + d\mathbf{u}$
        \item $c(d \mathbf{u}) = (cd)\mathbf{u}$
        \item $1\mathbf{u} = \mathbf{u}$
    \end{enumerate}
    The set of real $m \times n$ matrices is also a vector space.

    Let $V$ be the set of all functions $f: \mathbb{R} \rightarrow \mathbb{R}$. 
    Certain properties define a vector space over $V$:
    \begin{enumerate}
        \item Pointwise addition: $(f + g)(x) = f(x) + g(x)$
        \item Pointwise scalar multiplication: $(cf)(x) = cf(x)$
    \end{enumerate}
    There is also the complex vector space denoted $\mathbb{C}^n$.

    A subspace is a vector space if it is closed under addition and scalar multiplication, 
    for all other operations are inherited from the parent vector space. 
    For example, the set of $2 \times 2$ diagonal matrices is a subspace of the set of all $2 \times 2$ matrices,
    and the set of all polynomials is a subspace of the set of all real functions.

    Sets can be easily shown to not be subspaces by equating the zero vector to the set.

    \subsection*{4.2}
    Linear combinations and spans extend to vector spaces. 
    Checking if a vector is a linear combination of a set of vectors 
    is the same process for all vector spaces.
    For all vector spaces,
    \begin{enumerate}
        \item If $\mathbf{v_1}$ and $\mathbf{v_2}$ span a subspace, so does $k_2 \mathbf{v_1}$ and $k_2 \mathbf{v_2}$.
        \item If $\mathbf{v_1, v_2, v_3}$ span a subspace, so does $\mathbf{v_1, v_2, v_1 + v_2 + v_3}$.
    \end{enumerate}

    Any set of vectors from a vector space generate (span) a subspace.
    A vector can be shown to be in this subspace if it is a linear combination of the vectors spannig the subspace.
    
>>>>>>> 1fee37b855f3a8a47d08760ee9d1de989bb64555


\end{document} 