\documentclass{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathtools}

\newcommand\setItemnumber[1]{\setcounter{enumi}{\numexpr#1-1\relax}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title{Applied Linear Algebra Notes}
\author{Joe Hollander}
\date{Summer 2025}  

\begin{document}
\maketitle 

\section{}
    \subsection*{1.1}
    Elementary Row Operations
    \begin{enumerate}
        \item Interchange two rows.
        \item Multiply the elements of a row by a nonzero constant.
        \item Add a multiple of the elements of one row to the corresponding elements of another.
    \end{enumerate}
    Gauss-Jordan elimination is a systematic way to eliminate variables to arrive at a solution matrix. 
    The equation is solved when the matrix is diagonalized. Create zeros in each column until finished. 
    Finally, normalize the final element ((3,3) for a 3x3 matrix, etc.). 
    Interchange rows if a diagonal element is zero. Final matrix is called the reduced echelon form.
    The formula can be used for multiple constants with the same coefficient matrix. 
    To do this, just stack the constants in each column: 
    \begin{gather*}
    x - y + 3z = b_1 \\
    2x - y + 4z = b_2 \\
    -x + 2y -4z = b_3
    \end{gather*}
    
    for 
    \[
    \begin{bmatrix}
        b_1 \\
        b_2 \\
        b_3 \\
    \end{bmatrix}
    =
    \begin{bmatrix}
        8 \\
        11 \\
        -11 \\
    \end{bmatrix},
    \begin{bmatrix}
        0 \\
        1 \\
        2 \\
    \end{bmatrix},
    \begin{bmatrix}
        3 \\
        3 \\
        4 \\
    \end{bmatrix}
    \]
    would produce the augmented matrix
    \[
    \begin{bmatrix}
        1 & -1 & 3 & 8 & 0 & 3 \\
        2 & -1 & 4 & 11 & 1 & 3 \\
        -1 & 2 & -4 & -11 & 2 & 4 \\
    \end{bmatrix}
    \]
    which simplifies to 
    \[
    \begin{bmatrix}
        1 & 0 & 0 & 1 & 0 & -2 \\
        0 & 1 & 0 & -1 & 3 & 1 \\
        0 & 0 & 1 & 2 & 1 & 2 \\
    \end{bmatrix}.
    \]

    \subsection*{1.2}
    A matrix is in reduced echelon form if:
    \begin{enumerate}
        \item Any rows consisting entirely of zeros are grouped at the bottom of the matrix.
        \item The first nonzero element of each other row is 1. This element is called a leading 1. 
        \item The leading 1 of each row after the first is positioned to the right of the leading 1 of the previous row.
        \item All other elements in a column that contain a leading 1 are zero. 
    \end{enumerate}
    The reduced echelon of a matrix is unique. 
    To express multiple solutions, write the leading varaibles in each equation 
    (variables with the lowest subscript: $x_1$, etc.)  in terms of the remaining free variables.
    If last row has all zeros apart from constant, there are no solutions for the system.
    A system of linear equations is said to be homogeneous if all the constants are zero.  
    A homogeneous system of linear equations always has a solution of all zeros. 
    A homogenous system of linear equations that has more variables than equations (more rows than columns) 
    has many solutions. 

    \subsection*{1.7}
    Curve fitting a polynomial involves estimating variables, which can be done by solving a system of linear equations.
    Kirchhoff's Laws state:
    \begin{enumerate}
        \item All the current flowing into a junction must flow out of it
        \item The sum of the IR terms (I denotes current, R resistance) in any direction around a closed path
        is equal to the total voltage in the path in that direction. 
    \end{enumerate}
    Using these laws a system of linear equations can be formed to determine the current flowing through certain points.
    The same procedure can be used with traffic. 

    \subsection*{1.3}
    $\mathbb{R}^n$ is closed under addition and scalar multiplication if the result is also in $\mathbb{R}^n$. 
    To determine if $\mathbf{r}$ is a linear combination of $\mathbf{u, v,}$ and $\mathbf{w}$, 
    determine if this system has a solution: 
    \begin{gather*}
        c_1\mathbf{u_1} + c_2\mathbf{v_1} + c_3\mathbf{w_1} = \mathbf{r_1} \\
        c_1\mathbf{u_2} + c_2\mathbf{v_2} + c_3\mathbf{w_2} = \mathbf{r_2} \\
        c_1\mathbf{u_3} + c_2\mathbf{v_3} + c_3\mathbf{w_3} = \mathbf{r_3}.
    \end{gather*}
    The norm of a vector $\mathbf{r}$ is $\norm{\mathbf{r}}$, and it denotes the magnitude of a vector. 
    In $\mathbb{R}^n$, $\norm{\mathbf{r}} = \sqrt{\mathbf{r}^n}$.

    \subsection*{1.4}
    Subspaces are subsets of $\mathbb{R}^n$ that have all the same properties (subset must be closed under addition and multiplication).
    An example for demonstrating a subset in $\mathbb{R}^3$ with the form $(a, 2a, 3a)$ is closed for addition: 
    \begin{align*}
       (a, 2a, 3a) + (b, 2b, 3b) &= (a + b, 2a + 2b, 3a + 3b) \\
       &= (a + b, 2(a +b), 3(a + b)), 
    \end{align*}
    and multiplication: 
    \begin{align*}
        k(a, 2a, 3a) &= (ka, 2ka, 3ka) \\
        &= (ka, 2(ka), 3(ka)).
    \end{align*}


    When every vector of a vector space can be written as a linear combination of a set of vectors, the vectors span the space.
    For example, the subspace in $\mathbb{R}^n$ defined by $(a, b, a+b)$ is spanned by the vectors $(1, 0, 1)$ and $(0, 1, 1)$
    because $(a, b, a+b) = a(1, 0, 1) + b(0, 1, 1)$. Generally, vectors that make up a line in $\mathbb{R}^n$ are subspaces.
    The set of solutions to any homogenous system of linear equations is a subspace.

    \subsection*{1.5}
    The basis is the finite set of vectors that span a whole space. 
    The basis must also be linearly independent. 
    The vectors $\mathbf{v_1, v_2, \dots, v_n}$ are linearly independent 
    if the identity $c_1 \mathbf{v_1} + c_2 \mathbf{v_2} + \dots + c_n \mathbf{v_n} = \mathbf{0}$ 
    is only true for $c_1 = 0, c_2 = 0, \dots, c_n = 0$. 
    If the vectors are not linearly independent, they are linearly dependent. 
    Vectors can span the space but not be linearly independent. 
    For example, the vectors $(0, 1), (1, 0)$, and $(1, 1)$ span $\mathbb{R}^2$,
    but the set of the vectors is not a basis for $\mathbb{R}^2$ because they are not linearly independent.
    Additionally, $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$. 
    All subspaces must pass through the origin because they must contain the $\mathbf{0}$ vector. 

\section{}
    \subsection*{2.1}
    Matrix notation:
    \[
    A = 
    \begin{bmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33} \\
    \end{bmatrix}.
    \]
    The main diagonal consists of $a_{11}, a_{22}$, and $a_{33}$.
    Addition involves adding the corresponding elements of each matrix. 
    Both matrices must be the same size.
    Scalar multiplication involves multiplying every element. 
    Matrix multiplication ($C = AB$) requires that the number of columns in matrix $A$,
    is the same as the number of rows in matrix $B$. 
    The product is equal to the product of the rows of $A$ and the columns of $B$:
    \[
    c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \dots + a_{in}b_{nj}.
    \]
    For example, 
    \begin{align*}
    AB &= 
    \begin{bmatrix}
        1 & 3 \\
        2 & 0
    \end{bmatrix}
    \begin{bmatrix}
        5 & 0 & 1 \\
        3 & -2 & 6 
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 5 \\ 3 \end{bmatrix} & \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 0 \\ -2 \end{bmatrix} & \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 6 \end{bmatrix} \\
        \begin{bmatrix} 2 & 0 \end{bmatrix} \begin{bmatrix} 5 \\ 3 \end{bmatrix} & \begin{bmatrix} 2 & 0 \end{bmatrix} \begin{bmatrix} 0 \\ -2 \end{bmatrix} & \begin{bmatrix} 2 & 0 \end{bmatrix} \begin{bmatrix} 1 \\ 6 \end{bmatrix}
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        (1)(5) + (3)(3) & (1)(0) + (3)(-2) & (1)(1) + (3)(6) \\
        (2)(5) + (0)(3) & (2)(0) + (0)(-2) & (2)(1) + (0)(6)
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        14 & -6 & 19 \\
        10 & 0 & 2
    \end{bmatrix}.
    \end{align*}
    Matrix multiplication is not commumative, and $BA$ does not exist.
    If $A$ is an $m \times r$ matrix and $B$ is an $r \times n$ matrix, 
    then $AB$ is an $m \times n$ matrix.
    Three classes of matrices are notable: 
    The zero matrix which contains all zeros, 
    the diagonal matrix which contains zeros everywhere except the main diagonal,
    and the identity matrix which is a diagonal matrix with ones on the main diagonal.
    If $B_1$ is column one of matrix $B$,
    then $AB = [AB_1, AB_2, \dots, AB_n]$.
    Matrices can be partitioned into blocks, and these blocks can be multiplied.
    

\end{document}