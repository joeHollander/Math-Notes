\documentclass{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathtools}

\newcommand\setItemnumber[1]{\setcounter{enumi}{\numexpr#1-1\relax}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\colvec}{\begin{bmatrix} x \\ y \end{bmatrix}}

\title{Applied Linear Algebra Notes}
\author{Joe Hollander}
\date{Summer 2025}  

\begin{document}
\maketitle 

\section{}
    \subsection*{1.1}
    Elementary Row Operations
    \begin{enumerate}
        \item Interchange two rows.
        \item Multiply the elements of a row by a nonzero constant.
        \item Add a multiple of the elements of one row to the corresponding elements of another.
    \end{enumerate}
    Gauss-Jordan elimination is a systematic way to eliminate variables to arrive at a solution matrix. 
    The equation is solved when the matrix is diagonalized. Create zeros in each column until finished. 
    Finally, normalize the final element ((3,3) for a 3x3 matrix, etc.). 
    Interchange rows if a diagonal element is zero. Final matrix is called the reduced echelon form.
    The formula can be used for multiple constants with the same coefficient matrix. 
    To do this, just stack the constants in each column: 
    \begin{gather*}
    x - y + 3z = b_1 \\
    2x - y + 4z = b_2 \\
    -x + 2y -4z = b_3
    \end{gather*}
    
    for 
    \[
    \begin{bmatrix}
        b_1 \\
        b_2 \\
        b_3 \\
    \end{bmatrix}
    =
    \begin{bmatrix}
        8 \\
        11 \\
        -11 \\
    \end{bmatrix},
    \begin{bmatrix}
        0 \\
        1 \\
        2 \\
    \end{bmatrix},
    \begin{bmatrix}
        3 \\
        3 \\
        4 \\
    \end{bmatrix}
    \]
    would produce the augmented matrix
    \[
    \begin{bmatrix}
        1 & -1 & 3 & 8 & 0 & 3 \\
        2 & -1 & 4 & 11 & 1 & 3 \\
        -1 & 2 & -4 & -11 & 2 & 4 \\
    \end{bmatrix}
    \]
    which simplifies to 
    \[
    \begin{bmatrix}
        1 & 0 & 0 & 1 & 0 & -2 \\
        0 & 1 & 0 & -1 & 3 & 1 \\
        0 & 0 & 1 & 2 & 1 & 2 \\
    \end{bmatrix}.
    \]

    \subsection*{1.2}
    A matrix is in reduced echelon form if:
    \begin{enumerate}
        \item Any rows consisting entirely of zeros are grouped at the bottom of the matrix.
        \item The first nonzero element of each other row is 1. This element is called a leading 1. 
        \item The leading 1 of each row after the first is positioned to the right of the leading 1 of the previous row.
        \item All other elements in a column that contain a leading 1 are zero. 
    \end{enumerate}
    The reduced echelon of a matrix is unique. 
    To express multiple solutions, write the leading varaibles in each equation 
    (variables with the lowest subscript: $x_1$, etc.)  in terms of the remaining free variables.
    If last row has all zeros apart from constant, there are no solutions for the system.
    A system of linear equations is said to be homogeneous if all the constants are zero.  
    A homogeneous system of linear equations always has a solution of all zeros. 
    A homogenous system of linear equations that has more variables than equations (more rows than columns) 
    has many solutions. 

    \subsection*{1.7}
    Curve fitting a polynomial involves estimating variables, which can be done by solving a system of linear equations.
    Kirchhoff's Laws state:
    \begin{enumerate}
        \item All the current flowing into a junction must flow out of it
        \item The sum of the IR terms (I denotes current, R resistance) in any direction around a closed path
        is equal to the total voltage in the path in that direction. 
    \end{enumerate}
    Using these laws a system of linear equations can be formed to determine the current flowing through certain points.
    The same procedure can be used with traffic. 

    \subsection*{1.3}
    $\mathbb{R}^n$ is closed under addition and scalar multiplication if the result is also in $\mathbb{R}^n$. 
    To determine if $\mathbf{r}$ is a linear combination of $\mathbf{u, v,}$ and $\mathbf{w}$, 
    determine if this system has a solution: 
    \begin{gather*}
        c_1\mathbf{u_1} + c_2\mathbf{v_1} + c_3\mathbf{w_1} = \mathbf{r_1} \\
        c_1\mathbf{u_2} + c_2\mathbf{v_2} + c_3\mathbf{w_2} = \mathbf{r_2} \\
        c_1\mathbf{u_3} + c_2\mathbf{v_3} + c_3\mathbf{w_3} = \mathbf{r_3}.
    \end{gather*}
    The norm of a vector $\mathbf{r}$ is $\norm{\mathbf{r}}$, and it denotes the magnitude of a vector. 
    In $\mathbb{R}^n$, $\norm{\mathbf{r}} = \sqrt{\mathbf{r}^n}$.

    \subsection*{1.4}
    Subspaces are subsets of $\mathbb{R}^n$ that have all the same properties (subset must be closed under addition and multiplication).
    An example for demonstrating a subset in $\mathbb{R}^3$ with the form $(a, 2a, 3a)$ is closed for addition: 
    \begin{align*}
       (a, 2a, 3a) + (b, 2b, 3b) &= (a + b, 2a + 2b, 3a + 3b) \\
       &= (a + b, 2(a +b), 3(a + b)), 
    \end{align*}
    and multiplication: 
    \begin{align*}
        k(a, 2a, 3a) &= (ka, 2ka, 3ka) \\
        &= (ka, 2(ka), 3(ka)).
    \end{align*}


    When every vector of a vector space can be written as a linear combination of a set of vectors, the vectors span the space.
    For example, the subspace in $\mathbb{R}^n$ defined by $(a, b, a+b)$ is spanned by the vectors $(1, 0, 1)$ and $(0, 1, 1)$
    because $(a, b, a+b) = a(1, 0, 1) + b(0, 1, 1)$. Generally, vectors that make up a line in $\mathbb{R}^n$ are subspaces.
    The set of solutions to any homogenous system of linear equations is a subspace.

    \subsection*{1.5}
    The basis is the finite set of vectors that span a whole space. 
    The basis must also be linearly independent. 
    The vectors $\mathbf{v_1, v_2, \dots, v_n}$ are linearly independent 
    if the identity $c_1 \mathbf{v_1} + c_2 \mathbf{v_2} + \dots + c_n \mathbf{v_n} = \mathbf{0}$ 
    is only true for $c_1 = 0, c_2 = 0, \dots, c_n = 0$. 
    If the vectors are not linearly independent, they are linearly dependent. 
    Vectors can span the space but not be linearly independent. 
    For example, the vectors $(0, 1), (1, 0)$, and $(1, 1)$ span $\mathbb{R}^2$,
    but the set of the vectors is not a basis for $\mathbb{R}^2$ because they are not linearly independent.
    Additionally, $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$. 
    All subspaces must pass through the origin because they must contain the $\mathbf{0}$ vector. 

\section{}
    \subsection*{2.1}
    Matrix notation:
    \[
    A = 
    \begin{bmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33} \\
    \end{bmatrix}.
    \]
    The main diagonal consists of $a_{11}, a_{22}$, and $a_{33}$.
    Addition involves adding the corresponding elements of each matrix. 
    Both matrices must be the same size.
    Scalar multiplication involves multiplying every element. 
    Matrix multiplication ($C = AB$) requires that the number of columns in matrix $A$,
    is the same as the number of rows in matrix $B$. 
    The product is equal to the product of the rows of $A$ and the columns of $B$:
    \[
    c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \dots + a_{in}b_{nj}.
    \]
    For example, 
    \begin{align*}
    AB &= 
    \begin{bmatrix}
        1 & 3 \\
        2 & 0
    \end{bmatrix}
    \begin{bmatrix}
        5 & 0 & 1 \\
        3 & -2 & 6 
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 5 \\ 3 \end{bmatrix} & \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 0 \\ -2 \end{bmatrix} & \begin{bmatrix} 1 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 6 \end{bmatrix} \\
        \begin{bmatrix} 2 & 0 \end{bmatrix} \begin{bmatrix} 5 \\ 3 \end{bmatrix} & \begin{bmatrix} 2 & 0 \end{bmatrix} \begin{bmatrix} 0 \\ -2 \end{bmatrix} & \begin{bmatrix} 2 & 0 \end{bmatrix} \begin{bmatrix} 1 \\ 6 \end{bmatrix}
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        (1)(5) + (3)(3) & (1)(0) + (3)(-2) & (1)(1) + (3)(6) \\
        (2)(5) + (0)(3) & (2)(0) + (0)(-2) & (2)(1) + (0)(6)
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        14 & -6 & 19 \\
        10 & 0 & 2
    \end{bmatrix}.
    \end{align*}
    Matrix multiplication is not commumative, and $BA$ does not exist.
    If $A$ is an $m \times r$ matrix and $B$ is an $r \times n$ matrix, 
    then $AB$ is an $m \times n$ matrix.
    Three classes of matrices are notable: 
    The zero matrix $O$ which contains all zeros, 
    the diagonal matrix which contains zeros everywhere except the main diagonal,
    and the identity matrix $I$ which is a diagonal matrix with ones on the main diagonal.
    If $B_1$ is column one of matrix $B$,
    then $AB = [AB_1, AB_2, \dots, AB_n]$.
    Matrices can be partitioned into blocks, and these blocks can be multiplied.

    \subsection*{2.2}
    For matrix multiplication,
    \begin{enumerate}
        \item $A(BC) = (AB)C$
        \item $A(B + C) = AB + AC$
        \item $(A + B)C = AC + BC$
    \end{enumerate}
    The product of a chain of matrices will have the same number of rows as the first matrix 
    and the same number of columns as the last matrix. 
    For matrices, $AB = AC$ doesn't imply $B = C$, and
    $PQ = O$ doesn't imply $P = O$ or $Q = O$, 
    If $A$ is an $n \times n$ square matrix, and $r$ and $s$ are nonnegative integers, then
    \begin{enumerate}
        \item $A^rA^s = A^{r+s}$
        \item $(A^r)^s = A^{rs}$
        \item $A^0 = I_n$
    \end{enumerate}

    Consider a homogenous system of linear equations $AX = 0$. 
    Let $X_1$ and $X_2$ be solutions of the system.
    Then $AX_1 = 0$ and $AX_2 = 0$, so $AX_1 + AX_2 = A(X_1 + X_2) = 0$, and $X_1 + X_2$ is also a solution.
    Furthermore, if $c$ is any scalar, then $AX_1 = 0$ and $c(AX_1) = A(cX_1) = 0$, so $cX_1$ is also a solution.

    The corresponding homogeneous system of a matrix is one in which the right-hand side of the equations is zero,
    and the solution of the corresponding nonhomogeneous system is the solution of the homogeneous system plus a particular solution of the nonhomogeneous system.

    \subsection*{2.3}
    The transpose of a matrix $A$, denoted $A^T$, is the matrix whose columns are the rows of the given matrix $A$.
    Let $A$ and $B$ be matrices and $c$ be a scalar. There are several properties of the transpose:
    \begin{enumerate}
        \item $(A + B)^T = A^T + B^T$
        \item $(cA)^T = cA^T$
        \item $(AB)^T = B^TA^T$
        \item $(A^T)^T = A$
    \end{enumerate}
    A symmetrical matrix is a matrix that is equal to its transpose.
    The trace of $A$, denotedenoted $tr(A)$, is the sum of the elements on the main diagonal.
    There are several properties of the trace:
    \begin{enumerate}
        \item $tr(A + B) = tr(A) + tr(B)$
        \item $tr(AB) = tr(BA)$
        \item $tr(cA) = ctr(A)$
        \item $tr(A^T) = tr(A)$
    \end{enumerate}
    Matrices can also contain complex numbers, 
    and the conjugate of a matrix $A$ is denoted $\bar{A}$.
    Furthermore, the conjugate transpose of a matrix $A$ is denoted $A^*$,
    and a square matrix $C$ is called Hermitian if $C = C^*$.
    There are several properties of the conjugate transpose:
    \begin{enumerate}
        \item $(A + B)^* = A^* + B^*$
        \item $(zA)^* = \bar{z}A^*$
        \item $(AB)^* = B^*A^*$
        \item $(A^*)^* = A$
    \end{enumerate}

    In archeological seriation, a matrix is used to represent the relative ages of artifacts,
    with the defintion 
    \[
    a_{ij} = \begin{cases}
        1 & \text{if grave} \ i \ \text{contains pottery type} \ j \\
        0 & \text{if grave} \ i \ \text{does not contain pottery type} \ j
    \end{cases}
    \] 
    The element $g_{ij}$ of the matrix $G = AA^T$ 
    is equal to the number of types of pottery commmon to both grave $i$ and grave $j$.
    Then, create a map of the ages of the graves by starting with a great element from this matrix,
    and create connections depending on the elements. 
    For example, consider the matrix 
    \[
    G = \begin{bmatrix}
        1 & 1 & 0 & 0 & 0 \\
        1 & 2 & 0 & 0 & 1 \\
        0 & 0 & 2 & 1 & 1 \\
        0 & 0 & 1 & 1 & 0 \\
        0 & 1 & 1 & 0 & 2
    \end{bmatrix}
    \]
    Since $g_{12} = 1$, graves 1 and 2 share one type of pottery, so they are connected, and our current map is $1-2$. 
    Similarly, $g_{25} = 1$, so graves 2 and 5 are connected, and our map is now $1-2-5$.
    Again, $g_{53} = 1$, so graves 5 and 3 are connected, and our map is now $1-2-5-3$.
    Finally, $g_{34} = 1$, so graves 3 and 4 are connected, and our final map is now $1-2-5-3-4$.
    This means that grave 1 is the oldest or that grave 4 is the oldest. 
    In a similar method, the matrix $P=A^TA$ can be constructed, 
    and its elements detail how many graves contain both the $i$th and $j$th types of pottery. 
    The chronological order of the pottery can then also be determined using a map. 

    \subsection*{2.4}
    Let $A$ be an $n \times n$ matrix. If a matrix $B$ can be found such that $AB = BA = I_n$, 
    then $A$ is said to be invertible, and $B$ is called the inverse of $A$.
    If $A$ is invertible, then the unique inverse of $A$ is denoted $A^{-1}$.
    $A^{-k}$ is defined as $(A^{-1})^k$. 
    The inverse of matrix can be found by solving the system of linear equations $AX = I_n$
    For example, to determine the inverse of the matrix
    \[
    A = 
    \begin{bmatrix}
        1 & -1 & -2 \\
        2 & -3 & -5 \\
        -1 & 3 & 5
    \end{bmatrix},
    \]
    determine the reduced echelon form of the augmented matrix
    \[
    [A:I_n] =
    \begin{bmatrix}
         1 & -1 & -2 & 1 & 0 & 0 \\
        2 & -3 & -5 & 0 & 1 & 1\\
        -1 & 3 & 5 & 0 & 0 & 1
    \end{bmatrix}.
    \]
    There are several properties of the matrix inverse:
    \begin{enumerate}
        \item $(A^{-1})^{-1} = A$
        \item $(cA)^-1 = \frac{1}{c}A^{-1}$
        \item $(AB)^{-1} = B^{-1}A^{-1}$
        \item $(A^n)^{-1} = (A^{-1})^n$
        \item $(A^T)^{-1} = (A^{-1})^T$
    \end{enumerate}

    Let $AX=Y$ be a system of $n$ linear equations in $n$ variables. 
    If $A$ is invertible, then $X = A^{-1}Y$. 
    This method is often not used, for it is inefficient. 

    An elementary matrix is one that can be obtained from the identity matrix by a single elementary row operation.
    There are three types of elementary matrices:
    \begin{enumerate}
        \item Interchange rows
        \item Multiply a row by a nonzero constant
        \item Add a multiple of one row to another row
    \end{enumerate}
    These matrices can be used to apply elementary row operations to matrices through matrix multiplication. 
    Each elementary matrix is square and invertible,
    and if $A$ and $B$ are row equivalent matrices (one can be obtained from the other by elementary row operations) 
    and $A$ is invertible, then $B$ is also invertible.
    These matrices are used in the LU factorization of a matrix.


    \subsection*{2.5}
    A transformation $T$ of $\mathbb{R}^n$ into $\mathbb{R}^m$ is a rule that assigns to each vector $\mathbf{u}$ in $\mathbb{R}^n$
    a unique vector $\mathbf{u}$ in $\mathbb{R}^m$, and is denoted by $T(\mathbf{u}) = \mathbf{v}$.
    $\mathbb{R}^n$ is the domain of $T$, and $\mathbb{R}^m$ is the codomain of $T$.
    There are three main types of transformations:
    \begin{enumerate}
        \item Dilation: $T(\colvec) = \begin{bmatrix} r & 0 \\ 0 & r \end{bmatrix}\colvec$
        \item Reflection: $T(\colvec) = \begin{bmatrix} \pm1 & 0 \\ 0 & \pm1 \end{bmatrix}\colvec$
        \item Rotation: $T(\colvec) = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}\colvec$
    \end{enumerate}
    Transformations can also be combined. 

    An orthogonal matrix $A$ is an invertible matrix that has the property $A^-1 = A^T$.
    An orthogonal transformation is one where $T(\mathbf{u}) = A\mathbf{u}$ for some orthogonal matrix $A$.
    Orthogonal transformations preserve norms, differences in angles, and distances (rigid motions).

    A translation is a transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ of the form 
    $T(\mathbf{u}) = \mathbf{u} + \mathbf{v}$, where $\mathbf{v}$ is a fixed vector.
    An affine transformation is a transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ of the form
    $T(\mathbf{u}) = A \mathbf{u} + \mathbf{v}$. 

    \subsection*{2.6}
    Linear transformations imply that $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$ and
    $T(c\mathbf{u}) = cT(\mathbf{u})$ for any vectors $\mathbf{u}$ and $\mathbf{v}$ and any scalar $c$.
    Affine transformations are not linear transformations.
    To prove that a transformation is linear, it is sufficient to show that the transformation preserves vector addition and scalar multiplication.
    Any linear transformation can be represented by its standard matrix
    a matrix whose columns are the images of the standard basis vectors: 
    \[A = [T(\mathbf{e}_1) \ \dots \ T(\mathbf{e}_n)]\]
    To determine the transformation matrix then, apply the transformation to the standard basis vectors.
    
    Typically, translations cannot be used in combination with linear transformations as a product.
    However if an extra dimension is added, the translation can be represented as a linear transformation.
    This is called an homogeneous coordinate system. 
    For example, a point is represented as 
    \[
    \begin{bmatrix}
        x \\
        y \\
        1
    \end{bmatrix},
    \]
    and a translation is represented as
    \[
    \begin{bmatrix}
        1 & 0 & a \\
        0 & 1 & b \\
        0 & 0 & 1
    \end{bmatrix},
    \]
    where $a$ and $b$ are the translation amounts.

    Fractals can be generated using multiple affine transformations given various probabilities. 
    These parameters can be written as rows of a iterated function system matrix.
    
    \subsection*{2.8}
    A stochastic matrix is a square matrix whose elements are probabilities and whose columns sum to 1.
    If $A$ and $B$ are stochastic matrices, then $AB$ is also a stochastic matrix.
    
    \section*{3}

    \subsection*{3.1}
    The determinant of a $2 \times 2$ matrix $A$ is denoted by $|A|$ or det($A$) and is given by 
    \[
    \begin{vmatrix}
        a_{11} & a_{12} \\
        a_{21} & a_{22}
    \end{vmatrix}
    = 
    a_{11}a_{22}-a_{12}a_{21}
    \]
    This determinant is given by the difference of the products of the two diagonals of the matrix. 

    Let $A$ be a square matrix. The minor of the element $a_{ij}$ is the determinant of the matrix 
    obtained by removing the $i$th row and $j$th column from $A$.
    The cofactor of the element $a_{ij}$ is denoted $C_{ij}$ and is given by
    \[
    C_{ij} = (-1)^{i+j}M_{ij}
    \]
    Now, a new definition. 
    The determinant of a square matrix is the sum of the products of the elements of the first row
    and their cofactors. For example, if $A$ is a $3 \times 3$ matrix,
    then $|A| = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13}.$
    More generally, the determinant of a square matrix is the sum of the products
    of the elements of any row or column and theri cofactors. 
    \[
    i\text{th row expansion:} \quad |A| = a_{i1}C_{i1} + a_{i2}C_{i2} + \dots + a_{in}C_{in} \\
    \]
    The computation in evaluating a determinant can be minimized 
    by expanding in terms of the row or column that contains the most 0's. 
    The determinant of a $3 \times 3$ matrix can also be found using diagonals. 
    \[
    \begin{vmatrix}
    a & b & c \\
    d & e & f \\
    g & h & i
    \end{vmatrix}
     = aei + bfg + cdh - ceg - bdi - afh
    \]



\end{document} 